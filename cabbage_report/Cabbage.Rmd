---
title: "Vegetables Prices Analysis in Kathmandu Valley: Cabbage"
author: "Emma Casalini, Student Code: 60382,
         Chenjie Chen, Student Code: 53505,
         Daniel Enriquez, Student Code: 60369"
date: "2024-04-20"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1, messages = FALSE)
library(readr)
library(tidyverse)
library(fpp3)
library(gridExtra)
library(readxl)
library(lmtest)
```

## Cabbage Price Forecast

## 1. Introduction
Forecasting prices of staple foods such as cabbage and potatoes is important because of its direct impact on a country's economy and society. Nepal, in this case, is a country with a large agricultural sector that contributes to a large part of the country's GDP. The price of staple foods affects not only farmers' incomes, but also the economy of the country. 
These Nepalese staple food prices are also important to forecast as they indicate consumer purchasing power and the general rate of inflation in the country. By predicting these prices one can promote informed decisions, reduce risks and promote economic stability. Forecasting prices for staple foods is and will always be important.

Main goal: accurately forecast the prices of key vegetables in Nepal over a specific time horizon.

Dataset:
The data set consists of vegetables in Nepal, where each vegetable represents a time series. The data is of a daily-nature, but it has been transformed into monthly data for the sake of this analysis and forecasting project. 


## 1.1 Data Preparation

Loading the data sets: fruits and vegetables prices in Nepal, as well as the Consumer Price Index (CPI) for Nepal. The fruits and vegetables table contains the average price for vegetables and fruits on a given day. For the time series analysis this will be aggregated to the month level by taking the average of the average daily prices in a particular month.

The generated tsibble looks like this.

```{r}
# Loading Datasets.
veggies <- read_csv("../data_sources/kalimati_tarkari_dataset.csv", 
                    col_types = cols(Date = col_date(format = "%Y-%m-%d")))

suppressWarnings({
  cpi <- read_excel("../data_sources/Nepal CPI Trading Economics.xlsx", 
                    col_types = c("date", "numeric"))
})

cpi <- cpi |>
  filter(Date >= '2013-08-01 00:00:00')

# tidy up cpi
base_index <- cpi$CPI[1]
cpi <- cpi |>
  mutate(Date = yearmonth(cpi$Date),
         CPI = CPI/base_index) |>
  select(c(Date, CPI))

suppressMessages({
  veggies <- veggies %>%
    filter(Unit == "Kg") %>%
    filter(Commodity %in% c("Tomato Big(Nepali)", "Potato Red",
                            "Onion Dry (Indian)", "Carrot(Local)",
                            "Cabbage(Local)")) %>%
    mutate(Date = yearmonth(Date)) %>%
    select(Commodity, Date, Average) %>%
    group_by(Date, Commodity) %>%
    summarize(Average = mean(Average)) %>%
    ungroup()
})

veggies_ts <- veggies |>
  as_tsibble(key = Commodity,
             index = Date)

head(veggies_ts, 5)
```

## 2. Time Series Graphics

This plot shows the average monthly price in Nepalese Rupees of 5 vegetables in Nepal.

```{r}
veggies_ts |>
  autoplot(Average) +
  labs(y="Nepalese Rupees",
       title="Average price per month of various vegetables in Nepal") +
  theme_bw()
```
The following plot shows the average monthly price of Cabbage (local) in Nepal. The graph shows some spikes around the end of every 2 years, indicating a cyclic behavior. We also see deep dips at the beginning of every year, which could indicate a seasonal behavior. Finding seasonal and cyclic behaviors in agricultural data is not too surprising because agriculture is often influenced by factors such as weather conditions as well as planting and harvesting seasons.
There is a slight positive trend, in the sense that the price seems to stabilize in an increase every 2 years. 

```{r}

Cabbage <- veggies_ts %>% 
  filter(Commodity == "Cabbage(Local)")

Cabbage %>% 
  autoplot(Average) +
  labs(y="Nepalese Rupee",
       title="Average Price of Cabbage(Local) Red in Nepal") +
  theme_bw()
```
```{r}

```

This plot shows the average price of cabbage in Nepal per month. Looking at the plot we can see that the month with the highest price, on average, is November, then the price decreases rapidly in December up until march. This confirms the pattern of the previous plot.

```{r}
Cabbage %>% 
  gg_subseries(Average) +
  labs(y="Nepalese Rupee",
       title="Average Price Cabbage local in Nepal") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

Looking at The ACF it also suggests that we have both seasonal and cyclic behavior in the cabbage data, since it is alternating between positive and negative significant lags, followed by a weakly significant lag at lag 12 (which is a seasonal lag).
Lag 1 and lag 2 are positive and significant lags. The positive significant autocorrelation at lags 1 and 2 indicates a strong positive correlation between adjacent observations. This is consistent with the presence of a seasonal pattern, where prices tend to be positively correlated with those of the previous one or two periods. 

The negative significant autocorrelation at lags 5, 6, and 7 suggests an opposite trend. This could indicate a counter-cyclical or counter-seasonal effect, where prices in these periods have a tendency to move in the opposite direction from the recent trend.The positive autocorrelation at lag 12 indicates that prices tend to be positively correlated with those of the same period in the previous year. Lag 6 can also be considered a seasonal lag in the context of agriculture (which is also significant).

```{r}

Cabbage %>% 
  ACF(Average, lag_max = 24) %>%  
  autoplot() +
  labs(title="Autocorrelation  - Cabbage") +
  theme_bw()

```

## 3. Time series decomposition
Looking at the previous plot of the cabbage there is no super clear indication that it would need any time of transformation. A mathematical transformation (like a log-transformation) is mainly used to reduce multiplicative data, something that we do not necessarily have here. 
We could also consider a Box-cox transformation. Box cox is used to normalize skewed data and get heterogeneity, we can check how heterogenic the data is to decide whether or not a box-cox is a good choice with the breusch-pagan test.

The breusch-pagan test suggests (somewhat weakly) that we do not have heteroskedasticity because the p-value of 0.2754 is greater than 0.05. Therefore, we fail to reject the null hypothesis at the 0.05 significance level. In other words, we cannot conclude that there is evidence of heteroscedasticity in the data.

So now we have both visually looked at the data and done a breush-pagan test, but we can also inspect the impact of the box-cox transformation itself.
Looking at before and after the box-cox transformation we see that is possibly did account for some of the variation, so to perform the box-cox would not be too out of order and we ultimately performed the transformation.
```{r}

Cabbage_constant_prices <- Cabbage |>
  inner_join(cpi, by = join_by(Date)) |>
  mutate(Average_constant_prices = Average / CPI) |>
  select(Date, Commodity, Average_constant_prices)
head(Cabbage_constant_prices, 5)

lm_model <- lm(Average_constant_prices ~ Date, data = Cabbage_constant_prices)
bptest(lm_model)

Cabbage_constant_prices %>%
  features(Average_constant_prices, features = guerrero) %>%
  pull(lambda_guerrero)-> lambda

```

In the following can be found the difference between the data with and without the box-cox transformation. As seen it did account for some variance, but ultimately we decided to mot keep the box-coxed transformed Cabbage data. 

```{r}
Cabbage_constant_prices %>% autoplot(Average_constant_prices)

Cabbage_constant_prices %>%
  autoplot(box_cox(Average_constant_prices, lambda))

transformed_cabbage <- Cabbage_constant_prices %>%
  mutate(Average_constant_prices_transformed = box_cox(Average_constant_prices, lambda))

```

Trying and evaluating different decomposition models for the cabbage data. The decomposition model who seemingly did the best is the X_13ARIMA_SEATS model. From the following plots it visually seem to capture more variability in the trend component, but looking at the residuals it might not be the optimal forecasting model for the cabbage data, since it still has 4 significant lags where 3 of them are seasonal.

```{r}


Cabbage_constant_prices %>%
  model(STL1 = STL(Average_constant_prices),
        STL2 = STL(Average_constant_prices ~ trend(window = 7) + season(window = 5), robust = TRUE),
        fixed = STL(Average_constant_prices ~ season(window = "periodic"), robust = TRUE),
        x11 = X_13ARIMA_SEATS(Average_constant_prices),
        seats = X_13ARIMA_SEATS(Average_constant_prices ~ seats()))%>%
  components() -> dcmpCabbage

dcmpCabbage %>% as_tsibble() %>% autoplot(trend)
dcmpCabbage %>% as_tsibble() %>% autoplot(season_adjust)
autoplot(dcmpCabbage)

Cabbage_X13 <- Cabbage_constant_prices %>%
  model(RW(Average_constant_prices ~ drift()))

Cabbage_X13 %>% gg_tsresiduals() 

```

## 4. The forecasterâ€™s toolbox

For forecasting we will evaluate 4 different models for the cabbage data. We start off by creating a training data set and checking which method model is best for the cabbage time series. We have a total of 94 rows and will hence use 40 rows for training. Looking at the RMSE we can see that the drift method is the best alternative, but the residuals are still not that great with 4 significant lags, whereas 2 are seasonal.

```{r}
train_cabbage <- Cabbage_constant_prices %>% slice(1:(n() - 54))

fit_cabbage <- train_cabbage %>%
  model(
    Naive = NAIVE(Average_constant_prices),
    Seasonal_Naive = SNAIVE(Average_constant_prices),
    Drift = RW(Average_constant_prices ~ drift()),
    Mean = MEAN(Average_constant_prices)
  )

accuracy_table <- fit_cabbage %>% accuracy()

Cabbage_drift <- transformed_cabbage %>% model(RW(Average_constant_prices ~ drift()))

Cabbage_drift %>% gg_tsresiduals()

```

The following plot shows the the plot of the forecasted models (12 month period). It is clear here that no model is overly good and that even the best model is bad, as evidenced by its poor residuals.

```{r}
cabbage_forecast <- fit_cabbage%>%
  forecast(h = 12)

accuracy(cabbage_forecast, transformed_cabbage) %>% 
  summarise(.model = .model[which.min(RMSE)],RMSE = min(RMSE))

accuracy(cabbage_forecast, transformed_cabbage) %>% 
  summarise(.model = .model[which.min(MAE)], MAE = min(MAE))

accuracy(cabbage_forecast, transformed_cabbage) %>% 
  summarise(.model = .model[which.min(MAPE)], MAPE = min(MAPE))

accuracy(cabbage_forecast, transformed_cabbage) %>% 
  summarise(.model = .model[which.min(RMSSE)], RMSSE = min(RMSSE))

fit_cabbage %>% 
  select(Seasonal_Naive) %>% 
  augment() %>% 
  ACF(.resid) %>% autoplot()

```

## 5. Exponential smoothing

We are now going to choose a ETS model for the Cabbage, We have already looked at the ACF and seen that the data is stationary, but with somewhat seasonal components, with this in mind we choose the ANA model, because we do not have trend, but we do have seasonality that does increase with the level of the time series.

But we can also try Holts-Winter and SES just to have something to compare with. Out of these 3 models, the one we picked out (the MNM) model seems to be the best. Out of these models it is concluded that the MNM-model is the best. It is also the model that the program choose.

The ANA-model seems fine, the residuals resembles white noise and are somewhat normally distributed.
```{r}
Cabbage_ExponentialSmoothing <- Cabbage_constant_prices %>% 
  model(MNM = ETS(Average_constant_prices ~ error("M") + trend("N") + season("M")),
        holt = ETS(Average_constant_prices ~ error("A") + trend("A") + season("N")),
        ses = ETS(Average_constant_prices ~ error("A") + trend("N") + season("N")),
        ETS = ETS(Average_constant_prices))

report(Cabbage_ExponentialSmoothing)

CabbageMNM <- Cabbage_constant_prices %>% model(ses = ETS(Average_constant_prices ~ error("M") + trend("N") + season("M")))

CabbageMNM %>% gg_tsresiduals()

CabbageMNM %>% 
  tidy()

Cabbage_ExponentialSmoothing %>% glance()

```

Just looking at the forecasts the RMSE, and residuals, we can tell that the the MNM model is better than the best simple model method, which is the RW (random walk with drift). 

```{r}
cabbage_forecast2 <- CabbageMNM %>%
  forecast(h = 12)

accuracy(cabbage_forecast2, Cabbage_constant_prices) %>% 
  summarise(.model = .model[which.min(RMSE)],RMSE = min(RMSE))

accuracy(cabbage_forecast2, Cabbage_constant_prices) %>% 
  summarise(.model = .model[which.min(MAE)], MAE = min(MAE))

accuracy(cabbage_forecast2, Cabbage_constant_prices) %>% 
  summarise(.model = .model[which.min(MAPE)], MAPE = min(MAPE))

accuracy(cabbage_forecast2, Cabbage_constant_prices) %>% 
  summarise(.model = .model[which.min(RMSSE)], RMSSE = min(RMSSE))
```

# Arima

Our data is stationary and needed 0 differencing.Looking at the ACF that we have 2 seasonal lags and 7 non seasonal lags.
In the PACF we have 4 lags, no seasonal lags. So we can take the easy route and take an ARIMA(4,0,0).
when we compare our choice of model and the model chosen by the auto, we see that we could not pick up.

Auto chose the model ARIMA(4,0,0)(2,0,0)[12]  which represents a seasonal ARIMA model with seasonal terms:
P=2: There are two seasonal autoregressive terms at lag 12.
D=0: No seasonal differencing is applied.
Q=0: There are no seasonal moving average terms.

```{r}

Cabbage_constant_prices %>% gg_tsdisplay((Average_constant_prices), plot_type = "partial")

cabbage_fit <- Cabbage_constant_prices %>% 
  model(arima400 = ARIMA(Average_constant_prices ~ pdq(4,0,0)),
    auto = ARIMA(Average_constant_prices, stepwise = FALSE, approx = FALSE))

glance(cabbage_fit) %>% 
         arrange(AICc) %>% 
         select(.model:BIC)

```
Evaluation of the two models.We can see that the arima400 have one significant lag in their residuals (the seasonal lag 18) whilst the arima model (auto) that took the seasonality into account managed to get white-noise residuals, making it the superior alternative. The residuals are also looking more normally distributed.

Ultimately, there are only two models throughout all these tests that managed to get white-noise residuals and that is the seasonal arima and the MNM-exponential smoothing model. Altough looking only on the residual plot, the ARIMA model seems to have slightly more normally distributed residuals. 

```{r}
cabbage_fit %>% 
  select(arima400) %>% 
  gg_tsresiduals()

cabbage_fit %>% 
  select(auto) %>% 
  gg_tsresiduals()
```
